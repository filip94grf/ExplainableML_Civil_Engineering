{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69faf1-a3f5-4216-bcab-4e55769eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\III GODINA\\AIC M21 CASOPIS\\MATLAB CODE\\1.PRIPREMLJENA BAZA PODATAKA\\FUNDAMENTAL PERIOD PYTHON.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c9f741-41cd-45bf-92be-03ce3fe39e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great it works!\n",
    "\n",
    "# Can you now provide directly in the same way to jupyter-lab updated script for: \n",
    "# Script 6 — Genetic Programming Symbolic Regression (GEP-style)\n",
    "# Script 5 — LASSO Polynomial Regression (sparse closed-form)\n",
    "# Script 4 — Additive Spline GAM (explicit equation)\n",
    "# Script 3 — Model Tree (piecewise linear equations per region)\n",
    "# Script 2 — MARS (py-earth) to get piecewise-linear equations\n",
    "# Script 1 — Symbolic Regression (PySR) to discover equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821b86e8-9fcd-4ecf-aa1b-4f861cb98b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Julia backend...\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    14.51      2.05259e+17       11         0.434636         0.437851      1.43m\n",
      "   1     7.63      8.09781e+06       11         0.430864          0.47016      1.62m\n",
      "   2     7.99      2.03494e+06       15         0.374694         0.353894      1.48m\n",
      "   3    11.12      2.63235e+06       10         0.370396         0.392771      1.61m\n",
      "   4    13.06      1.50242e+06       19         0.319865         0.334479      2.01m\n",
      "   5    13.76      1.83628e+06       19         0.272012         0.276733      1.46m\n",
      "   6    14.74           260153       28         0.254737         0.269243      1.44m\n",
      "   7    17.70           122907       28         0.253063         0.283074      1.70m\n",
      "   8    20.45           103393       15         0.195815         0.208182      1.78m\n",
      "   9    20.73          210.402       28         0.181048         0.181108      2.16m\n",
      "  10    20.44          113.911       28         0.180726         0.183976      1.51m\n",
      "  11    21.16          43317.4       23         0.161708         0.172288      1.65m\n",
      "  12    20.39          115.713       23         0.162071         0.169191      1.53m\n",
      "  13    20.67          92075.2       32         0.152891         0.171905      1.63m\n",
      "  14    21.88          61398.4       32         0.145062         0.147136      1.61m\n",
      "  15    22.23           153358       42         0.143039         0.131005      1.10m\n",
      "  16    23.26           146983       33         0.123012         0.126363      1.05m\n",
      "  17    25.14          43025.9       33         0.123778         0.119451      1.07m\n",
      "  18    26.91          118.269       33         0.122365         0.131883     56.18s\n",
      "  19    29.63           161826       59         0.121204          0.14321      1.08m\n",
      "  20    32.21          82.7699       32         0.120789         0.123926     52.40s\n",
      "  21    33.26      1.39772e+08       32           0.1209         0.122954     48.01s\n",
      "  22    33.26          28053.7       32         0.120185         0.129096     42.77s\n",
      "  23    32.77           196320       32         0.119119         0.137689     37.02s\n",
      "  24    32.92           158278       32         0.119958         0.130983     49.77s\n",
      "  25    32.50          106.713       32         0.119031         0.138371     31.10s\n",
      "  26    32.62           189298       32         0.118013         0.145994     19.10s\n",
      "  27    31.77          64839.9       32         0.118996         0.138638     11.01s\n",
      "  28    31.88           484995       32         0.118361         0.143443      5.45s\n",
      "  29    31.81          110.949       32         0.119268         0.136523      0.00s\n",
      "\n",
      "=== GP-Fallback — Symbolic Regression (original units) ===\n",
      "Train: {'R2': 0.980087, 'MAE': 0.09736, 'RMSE': 0.121115}\n",
      "Test : {'R2': 0.975861, 'MAE': 0.105933, 'RMSE': 0.133867}\n",
      "\n",
      "Closed-form (Python):\n",
      " y = psqrt((psqrt((plog((plog(pdiv(plog(LoSp), pdiv(NoSp, NoSt))) - (psqrt((1.721 + 3.539)) - plog(LoSp)))) * LoSp)) * pexp(((psqrt(NoSt) - (4.041 + -0.563)) * plog(plog(LoSp))))))\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y = \\left(\\log{\\left(\\left|{LoSp}\\right| + 1 \\right)} + 1\\right)^{\\frac{\\sqrt{\\left|{NoSt}\\right|}}{2} - 1.739} \\sqrt[4]{\\log{\\left(\\left|{\\log{\\left(\\log{\\left(\\left(\\left|{LoSp}\\right| + 1\\right)^{\\left|{\\frac{NoSt + 1.0 \\cdot 10^{-9}}{NoSp + 1.0 \\cdot 10^{-9} NoSt + 1.0 \\cdot 10^{-18}}}\\right|} \\right)} + 1 \\right)} + \\log{\\left(\\left|{LoSp}\\right| + 1 \\right)} - 2.29346898823594}\\right| + 1 \\right)}} \\sqrt[4]{\\left|{LoSp}\\right|}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts saved in: C:\\Users\\filip\\Documents\\POGLAVLJE KNJIGE\\out_symbolic_period_BARE\n",
      "Backend used: gplearn\n",
      "Running Time:  163.21102356910706  seconds\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER CELL — Script 1: Symbolic Regression with pretty math display\n",
    "# Dataset: X = ['NoSt','NoSp','LoSp','OP','MWS'], y = 'TFP' (original units)\n",
    "\n",
    "import os, math, json, warnings, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start = time.time()\n",
    "# ---------- CONFIG ----------\n",
    "#PROMENA\n",
    "EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\IIIII GODINA\\8.CSP - NOVA KNJIGA SA VM\\MOJE POGLAVLJE\\CASE STUDIES\\FUNDAMENTAL PERIOD BARE FRAMES Dataset.xlsx\"\n",
    "SHEET = 0\n",
    "#PROMENA\n",
    "FEATURES = [\"NoSt\",\"NoSp\",\"LoSp\"]\n",
    "#PROMENA\n",
    "TARGET = \"TFP\"\n",
    "\n",
    "TEST_SIZE   = 0.20\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# PySR knobs (used only if PySR+Julia available)\n",
    "PYSR_NITER     = 80\n",
    "PYSR_MAXSIZE   = 30\n",
    "PYSR_MAX_EVALS = 500\n",
    "PYSR_BIN_OPS   = [\"+\", \"-\", \"*\", \"/\", \"^\"]\n",
    "PYSR_UN_OPS    = [\"sqrt\"]\n",
    "PYSR_CONSTRAINTS = {\"^\": (5, 1)}\n",
    "\n",
    "# gplearn (fallback) knobs\n",
    "GP_POP_SIZE   = 3000\n",
    "GP_GENS       = 30\n",
    "GP_TOURN_SIZE = 20\n",
    "GP_PARSIMONY  = 0.001\n",
    "GP_CONST_MIN, GP_CONST_MAX = -5.0, 5.0\n",
    "\n",
    "#PROMENA\n",
    "OUTDIR = \"out_symbolic_period_BARE\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "missing = [c for c in FEATURES + [TARGET] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in Excel: {missing}\\nPresent: {list(df.columns)}\")\n",
    "\n",
    "X = df[FEATURES].apply(pd.to_numeric, errors=\"coerce\").values\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\").values\n",
    "mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        RMSE=math.sqrt(((y_true - y_pred)**2).mean()),\n",
    "    )\n",
    "\n",
    "# ---------- Pretty math display (SymPy) ----------\n",
    "from sympy import sympify, Abs, sqrt, log, exp, latex, simplify, symbols\n",
    "from IPython.display import Math, display\n",
    "\n",
    "def show_math_equation(eq_py: str, var_order):\n",
    "    \"\"\"\n",
    "    Render 'y = <expr>' in proper math notation using SymPy.\n",
    "    Maps:\n",
    "      pdiv(a,b) -> a/(b + 1e-9)\n",
    "      psqrt(x)  -> sqrt(|x|)\n",
    "      plog(x)   -> log(1 + |x|)\n",
    "      pexp(x)   -> exp(x)\n",
    "    \"\"\"\n",
    "    # Create SymPy symbols for variables\n",
    "    syms = symbols(\" \".join(var_order), real=True)\n",
    "    loc = {name: sym for name, sym in zip(var_order, syms)}\n",
    "    # Map protected functions to SymPy expressions\n",
    "    loc.update({\n",
    "        \"pdiv\": lambda a, b: a/(b + 1e-9),\n",
    "        \"psqrt\": lambda x: sqrt(Abs(x)),\n",
    "        \"plog\": lambda x: log(1 + Abs(x)),\n",
    "        \"pexp\": lambda x: exp(x),\n",
    "    })\n",
    "    try:\n",
    "        expr = sympify(eq_py, locals=loc)\n",
    "        expr = simplify(expr)\n",
    "        display(Math(r\"y = \" + latex(expr)))\n",
    "    except Exception as e:\n",
    "        print(\"Could not render pretty math; showing raw expression instead.\")\n",
    "        print(\"y =\", eq_py)\n",
    "\n",
    "def save_results(name, eq_py, yhat_tr, yhat_te):\n",
    "    mt = metrics(y_te, yhat_te); mtr = metrics(y_tr, yhat_tr)\n",
    "    print(f\"\\n=== {name} — Symbolic Regression (original units) ===\")\n",
    "    print(\"Train:\", {k: round(v, 6) for k, v in mtr.items()})\n",
    "    print(\"Test :\", {k: round(v, 6) for k, v in mt.items()})\n",
    "    print(\"\\nClosed-form (Python):\\n\", \"y = \" + eq_py)\n",
    "    # Pretty math:\n",
    "    show_math_equation(eq_py, FEATURES)\n",
    "\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_equation_python.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"y = \" + eq_py + \"\\n\")\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train\": mtr, \"test\": mt}, f, indent=2)\n",
    "\n",
    "used_backend = None\n",
    "\n",
    "# ---------- Try PySR first ----------\n",
    "try:\n",
    "    from pysr import PySRRegressor\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        niterations=PYSR_NITER,\n",
    "        maxsize=PYSR_MAXSIZE,\n",
    "        populations=30,\n",
    "        population_size=60,\n",
    "        parsimony=1e-4,\n",
    "        progress=True,\n",
    "        binary_operators=PYSR_BIN_OPS,\n",
    "        unary_operators=PYSR_UN_OPS,\n",
    "        loss=\"L2DistLoss()\",\n",
    "        model_selection=\"best\",\n",
    "        max_evals=PYSR_MAX_EVALS,\n",
    "        random_state=RANDOM_SEED,\n",
    "        constraints=PYSR_CONSTRAINTS,\n",
    "    ).fit(X_tr, y_tr, variable_names=FEATURES)\n",
    "\n",
    "    # Pick simplest within 5% of best test MAE\n",
    "    candidates = []\n",
    "    for _, row in model.equations_.iterrows():\n",
    "        # 'equation' is a Python-evaluable string produced by PySR (in terms of variable names)\n",
    "        eq_py = row[\"equation\"]\n",
    "        # Evaluate this candidate\n",
    "        f = model.lambda_format()(row[\"equation\"])\n",
    "        yhat_tr = np.array(f(X_tr), float)\n",
    "        yhat_te = np.array(f(X_te), float)\n",
    "        candidates.append({\n",
    "            \"complexity\": int(row[\"complexity\"]),\n",
    "            \"py\": eq_py,\n",
    "            \"train\": metrics(y_tr, yhat_tr),\n",
    "            \"test\":  metrics(y_te, yhat_te),\n",
    "            \"yhat_tr\": yhat_tr, \"yhat_te\": yhat_te\n",
    "        })\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"PySR returned no parsable equations.\")\n",
    "\n",
    "    best_mae = min(c[\"test\"][\"MAE\"] for c in candidates)\n",
    "    pool = [c for c in candidates if c[\"test\"][\"MAE\"] <= 1.05*best_mae]\n",
    "    best = sorted(pool, key=lambda c: (c[\"complexity\"], c[\"test\"][\"MAE\"]))[0]\n",
    "\n",
    "    used_backend = \"PySR\"\n",
    "    save_results(\"PySR\", best[\"py\"], best[\"yhat_tr\"], best[\"yhat_te\"])\n",
    "\n",
    "except Exception:\n",
    "    # ---------- Fallback: gplearn (pure Python) ----------\n",
    "    from gplearn.genetic import SymbolicRegressor\n",
    "    from gplearn.functions import make_function\n",
    "\n",
    "    def _pdiv(x, y):\n",
    "        return np.divide(x, np.where(np.abs(y) < 1e-9, np.sign(y)*1e-9 + (y==0)*1e-9, y))\n",
    "    def _psqrt(x):\n",
    "        return np.sqrt(np.abs(x))\n",
    "    def _plog(x):\n",
    "        return np.log1p(np.abs(x))\n",
    "    def _pexp(x):\n",
    "        return np.exp(np.clip(x, -20, 20))\n",
    "\n",
    "    pdiv = make_function(function=_pdiv, name=\"pdiv\", arity=2)\n",
    "    psqrt = make_function(function=_psqrt, name=\"psqrt\", arity=1)\n",
    "    plog  = make_function(function=_plog,  name=\"plog\",  arity=1)\n",
    "    pexp  = make_function(function=_pexp,  name=\"pexp\",  arity=1)\n",
    "\n",
    "    gp = SymbolicRegressor(\n",
    "        function_set=(\"add\",\"sub\",\"mul\",pdiv,psqrt,plog,pexp),\n",
    "        metric=\"rmse\",\n",
    "        population_size=GP_POP_SIZE,\n",
    "        generations=GP_GENS,\n",
    "        tournament_size=GP_TOURN_SIZE,\n",
    "        const_range=(GP_CONST_MIN, GP_CONST_MAX),\n",
    "        init_depth=(2,6),\n",
    "        init_method=\"half and half\",\n",
    "        p_crossover=0.8,\n",
    "        p_subtree_mutation=0.01,\n",
    "        p_hoist_mutation=0.01,\n",
    "        p_point_mutation=0.08,\n",
    "        parsimony_coefficient=GP_PARSIMONY,\n",
    "        max_samples=0.9,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    gp.fit(X_tr, y_tr)\n",
    "\n",
    "    # Convert gplearn program string to a Python-evaluable expression with your feature names\n",
    "    def program_to_python(expr_str, var_names):\n",
    "        s = expr_str\n",
    "        for i, n in enumerate(var_names):\n",
    "            s = re.sub(rf\"\\bX{i}\\b\", n, s)\n",
    "        s = s.replace(\"add(\", \"ADD(\").replace(\"sub(\", \"SUB(\").replace(\"mul(\", \"MUL(\")\n",
    "        def bin_to_infix(text, token, op):\n",
    "            while token in text:\n",
    "                idx = text.find(token + \"(\")\n",
    "                if idx == -1: break\n",
    "                depth, j, comma = 0, idx + len(token) + 1, None\n",
    "                while j < len(text):\n",
    "                    if text[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif text[j] == \")\":\n",
    "                        if depth == 0: break\n",
    "                        depth -= 1\n",
    "                    elif text[j] == \",\" and depth == 0:\n",
    "                        comma = j; break\n",
    "                    j += 1\n",
    "                depth2, k = 0, comma + 1\n",
    "                while k < len(text):\n",
    "                    if text[k] == \"(\":\n",
    "                        depth2 += 1\n",
    "                    elif text[k] == \")\":\n",
    "                        if depth2 == 0: break\n",
    "                        depth2 -= 1\n",
    "                    k += 1\n",
    "                a = text[idx + len(token) + 1:comma]\n",
    "                b = text[comma + 1:k]\n",
    "                repl = \"(\" + a.strip() + f\" {op} \" + b.strip() + \")\"\n",
    "                text = text[:idx] + repl + text[k+1:]\n",
    "            return text\n",
    "        s = bin_to_infix(s, \"ADD\", \"+\")\n",
    "        s = bin_to_infix(s, \"SUB\", \"-\")\n",
    "        s = bin_to_infix(s, \"MUL\", \"*\")\n",
    "        return s\n",
    "\n",
    "    expr_str = str(gp._program)\n",
    "    eq_py = program_to_python(expr_str, FEATURES)\n",
    "\n",
    "    # Evaluate safely\n",
    "    def safe_eval_py(expr, Xmat, names):\n",
    "        env = {\n",
    "            \"np\": np,\n",
    "            \"pdiv\": lambda a,b: a/np.where(np.abs(b)<1e-9, np.sign(b)*1e-9 + (b==0)*1e-9, b),\n",
    "            \"psqrt\": lambda x: np.sqrt(np.abs(x)),\n",
    "            \"plog\":  lambda x: np.log1p(np.abs(x)),\n",
    "            \"pexp\":  lambda x: np.exp(np.clip(x, -20, 20)),\n",
    "        }\n",
    "        vals = {n: Xmat[:, i] for i, n in enumerate(names)}\n",
    "        return np.asarray(eval(expr, env, vals), float)\n",
    "\n",
    "    yhat_tr = safe_eval_py(eq_py, X_tr, FEATURES)\n",
    "    yhat_te = safe_eval_py(eq_py, X_te, FEATURES)\n",
    "\n",
    "    used_backend = \"gplearn\"\n",
    "    save_results(\"GP-Fallback\", eq_py, yhat_tr, yhat_te)\n",
    "\n",
    "print(f\"\\nArtifacts saved in: {os.path.abspath(OUTDIR)}\")\n",
    "print(\"Backend used:\", used_backend or \"unknown\")\n",
    "end = time.time()\n",
    "running_time = (end - start)\n",
    "print('Running Time: ', running_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36be204-72b7-426a-b328-1bdc3d25762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Controlling simplicity vs accuracy\n",
    "\n",
    "#Increase --max_evals / --niterations to search longer (find more accurate/possibly more complex formulas).\n",
    "\n",
    "#Lower --maxsize or raise parsimony (inside the script) to prefer simpler equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
