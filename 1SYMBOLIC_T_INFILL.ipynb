{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69faf1-a3f5-4216-bcab-4e55769eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\III GODINA\\AIC M21 CASOPIS\\MATLAB CODE\\1.PRIPREMLJENA BAZA PODATAKA\\FUNDAMENTAL PERIOD PYTHON.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c9f741-41cd-45bf-92be-03ce3fe39e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great it works!\n",
    "\n",
    "# Can you now provide directly in the same way to jupyter-lab updated script for: \n",
    "# Script 6 — Genetic Programming Symbolic Regression (GEP-style)\n",
    "# Script 5 — LASSO Polynomial Regression (sparse closed-form)\n",
    "# Script 4 — Additive Spline GAM (explicit equation)\n",
    "# Script 3 — Model Tree (piecewise linear equations per region)\n",
    "# Script 2 — MARS (py-earth) to get piecewise-linear equations\n",
    "# Script 1 — Symbolic Regression (PySR) to discover equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821b86e8-9fcd-4ecf-aa1b-4f861cb98b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Julia backend...\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    12.94       3.8657e+24        6         0.400829         0.427693      1.80m\n",
      "   1     7.67       4.3131e+12        6         0.399085         0.442058      1.59m\n",
      "   2     6.82      1.23393e+07        6         0.398476         0.446947      1.65m\n",
      "   3     7.08      3.66746e+07       10         0.352329         0.328586      1.92m\n",
      "   4     6.60      9.08677e+07       10         0.348086          0.36685      1.41m\n",
      "   5     6.18      1.69154e+08       11         0.301153         0.321702      1.32m\n",
      "   6     6.60      1.23409e+08       11         0.301208         0.321242      1.24m\n",
      "   7     8.59      7.64783e+12       11         0.299392         0.336093      1.42m\n",
      "   8    11.07      2.81013e+07       19           0.2967         0.339606      1.88m\n",
      "   9    12.21      1.30327e+10       12         0.270363         0.292606      1.96m\n",
      "  10    11.91      1.24787e+09       12         0.267957         0.311794      1.61m\n",
      "  11    12.13      1.19917e+10       12          0.25883         0.263386      1.31m\n",
      "  12    12.86      4.00168e+13       12         0.236861          0.26726      1.52m\n",
      "  13    12.33      8.24354e+07       12          0.23246         0.264635      1.77m\n",
      "  14    11.99      6.41978e+07       12          0.22849         0.235354      1.34m\n",
      "  15    11.93      4.96016e+07       12         0.225919         0.256618     56.34s\n",
      "  16    11.91      5.92642e+07       12         0.222307         0.231038      1.05m\n",
      "  17    12.04      6.72561e+07       12         0.219991         0.250097     47.82s\n",
      "  18    11.93      6.50184e+07       12         0.219389          0.25479     54.11s\n",
      "  19    11.93      3.98266e+07       12         0.218858          0.25885     40.60s\n",
      "  20    11.93      6.49857e+07       12          0.21786         0.266277     49.76s\n",
      "  21    11.86      3.09065e+08       12         0.218784         0.259405     35.49s\n",
      "  22    12.01      6.78696e+07       12         0.218829         0.259064     28.94s\n",
      "  23    11.99       2.1974e+08       12         0.217286          0.27045     28.97s\n",
      "  24    11.95      6.50838e+07       12         0.218019         0.265112     21.98s\n",
      "  25    11.78       1.3752e+11       12         0.218685         0.260158     18.22s\n",
      "  26    12.09      6.15789e+07       12         0.217336         0.260075     19.17s\n",
      "  27    11.91      7.48237e+07       12         0.217728         0.267247     12.33s\n",
      "  28    11.93      4.59266e+07       15         0.188766         0.195038      5.62s\n",
      "  29    12.02      1.72029e+09       15         0.187164         0.208404      0.00s\n",
      "\n",
      "=== GP-Fallback — Symbolic Regression (original units) ===\n",
      "Train: {'R2': 0.898269, 'MAE': 0.138588, 'RMSE': 0.189591}\n",
      "Test : {'R2': 0.868728, 'MAE': 0.147033, 'RMSE': 0.203489}\n",
      "\n",
      "Closed-form (Python):\n",
      " y = (pexp(-3.281) * (NoSt * pdiv(plog((OP + -4.958)), plog(plog((3.711 * MWS))))))\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y = \\frac{0.0375906473582276 NoSt \\log{\\left(\\left|{OP - 4.958}\\right| + 1 \\right)}}{\\log{\\left(\\log{\\left(3.711 \\left|{MWS}\\right| + 1 \\right)} + 1 \\right)} + 1.0 \\cdot 10^{-9}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts saved in: C:\\Users\\filip\\Documents\\POGLAVLJE KNJIGE\\out_symbolic_period_infill\n",
      "Backend used: gplearn\n",
      "Running Time:  141.85540533065796  seconds\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER CELL — Script 1: Symbolic Regression with pretty math display\n",
    "# Dataset: X = ['NoSt','NoSp','LoSp','OP','MWS'], y = 'TFP' (original units)\n",
    "\n",
    "import os, math, json, warnings, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\IIIII GODINA\\8.CSP - NOVA KNJIGA SA VM\\MOJE POGLAVLJE\\CASE STUDIES\\FUNDAMENTAL PERIOD INFILL FRAMES Dataset.xlsx\"\n",
    "SHEET = 0\n",
    "FEATURES = [\"NoSt\",\"NoSp\",\"LoSp\",\"OP\",\"MWS\"]\n",
    "TARGET = \"TFP\"\n",
    "\n",
    "TEST_SIZE   = 0.20\n",
    "RANDOM_SEED = 42\n",
    "start = time.time()\n",
    "# PySR knobs (used only if PySR+Julia available)\n",
    "PYSR_NITER     = 50\n",
    "PYSR_MAXSIZE   = 30\n",
    "PYSR_MAX_EVALS = 800\n",
    "PYSR_BIN_OPS   = [\"+\", \"-\", \"*\", \"/\", \"^\"]\n",
    "PYSR_UN_OPS    = [\"sqrt\"]\n",
    "PYSR_CONSTRAINTS = {\"^\": (5, 1)}\n",
    "\n",
    "# gplearn (fallback) knobs\n",
    "GP_POP_SIZE   = 3000\n",
    "GP_GENS       = 30\n",
    "GP_TOURN_SIZE = 20\n",
    "GP_PARSIMONY  = 0.001\n",
    "GP_CONST_MIN, GP_CONST_MAX = -5.0, 5.0\n",
    "\n",
    "OUTDIR = \"out_symbolic_period_infill\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "missing = [c for c in FEATURES + [TARGET] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in Excel: {missing}\\nPresent: {list(df.columns)}\")\n",
    "\n",
    "X = df[FEATURES].apply(pd.to_numeric, errors=\"coerce\").values\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\").values\n",
    "mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        RMSE=math.sqrt(((y_true - y_pred)**2).mean()),\n",
    "    )\n",
    "\n",
    "# ---------- Pretty math display (SymPy) ----------\n",
    "from sympy import sympify, Abs, sqrt, log, exp, latex, simplify, symbols\n",
    "from IPython.display import Math, display\n",
    "\n",
    "def show_math_equation(eq_py: str, var_order):\n",
    "    \"\"\"\n",
    "    Render 'y = <expr>' in proper math notation using SymPy.\n",
    "    Maps:\n",
    "      pdiv(a,b) -> a/(b + 1e-9)\n",
    "      psqrt(x)  -> sqrt(|x|)\n",
    "      plog(x)   -> log(1 + |x|)\n",
    "      pexp(x)   -> exp(x)\n",
    "    \"\"\"\n",
    "    # Create SymPy symbols for variables\n",
    "    syms = symbols(\" \".join(var_order), real=True)\n",
    "    loc = {name: sym for name, sym in zip(var_order, syms)}\n",
    "    # Map protected functions to SymPy expressions\n",
    "    loc.update({\n",
    "        \"pdiv\": lambda a, b: a/(b + 1e-9),\n",
    "        \"psqrt\": lambda x: sqrt(Abs(x)),\n",
    "        \"plog\": lambda x: log(1 + Abs(x)),\n",
    "        \"pexp\": lambda x: exp(x),\n",
    "    })\n",
    "    try:\n",
    "        expr = sympify(eq_py, locals=loc)\n",
    "        expr = simplify(expr)\n",
    "        display(Math(r\"y = \" + latex(expr)))\n",
    "    except Exception as e:\n",
    "        print(\"Could not render pretty math; showing raw expression instead.\")\n",
    "        print(\"y =\", eq_py)\n",
    "\n",
    "def save_results(name, eq_py, yhat_tr, yhat_te):\n",
    "    mt = metrics(y_te, yhat_te); mtr = metrics(y_tr, yhat_tr)\n",
    "    print(f\"\\n=== {name} — Symbolic Regression (original units) ===\")\n",
    "    print(\"Train:\", {k: round(v, 6) for k, v in mtr.items()})\n",
    "    print(\"Test :\", {k: round(v, 6) for k, v in mt.items()})\n",
    "    print(\"\\nClosed-form (Python):\\n\", \"y = \" + eq_py)\n",
    "    # Pretty math:\n",
    "    show_math_equation(eq_py, FEATURES)\n",
    "\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_equation_python.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"y = \" + eq_py + \"\\n\")\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train\": mtr, \"test\": mt}, f, indent=2)\n",
    "\n",
    "used_backend = None\n",
    "\n",
    "# ---------- Try PySR first ----------\n",
    "try:\n",
    "    from pysr import PySRRegressor\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        niterations=PYSR_NITER,\n",
    "        maxsize=PYSR_MAXSIZE,\n",
    "        populations=30,\n",
    "        population_size=60,\n",
    "        parsimony=1e-4,\n",
    "        progress=True,\n",
    "        binary_operators=PYSR_BIN_OPS,\n",
    "        unary_operators=PYSR_UN_OPS,\n",
    "        loss=\"L2DistLoss()\",\n",
    "        model_selection=\"best\",\n",
    "        max_evals=PYSR_MAX_EVALS,\n",
    "        random_state=RANDOM_SEED,\n",
    "        constraints=PYSR_CONSTRAINTS,\n",
    "    ).fit(X_tr, y_tr, variable_names=FEATURES)\n",
    "\n",
    "    # Pick simplest within 5% of best test MAE\n",
    "    candidates = []\n",
    "    for _, row in model.equations_.iterrows():\n",
    "        # 'equation' is a Python-evaluable string produced by PySR (in terms of variable names)\n",
    "        eq_py = row[\"equation\"]\n",
    "        # Evaluate this candidate\n",
    "        f = model.lambda_format()(row[\"equation\"])\n",
    "        yhat_tr = np.array(f(X_tr), float)\n",
    "        yhat_te = np.array(f(X_te), float)\n",
    "        candidates.append({\n",
    "            \"complexity\": int(row[\"complexity\"]),\n",
    "            \"py\": eq_py,\n",
    "            \"train\": metrics(y_tr, yhat_tr),\n",
    "            \"test\":  metrics(y_te, yhat_te),\n",
    "            \"yhat_tr\": yhat_tr, \"yhat_te\": yhat_te\n",
    "        })\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"PySR returned no parsable equations.\")\n",
    "\n",
    "    best_mae = min(c[\"test\"][\"MAE\"] for c in candidates)\n",
    "    pool = [c for c in candidates if c[\"test\"][\"MAE\"] <= 1.05*best_mae]\n",
    "    best = sorted(pool, key=lambda c: (c[\"complexity\"], c[\"test\"][\"MAE\"]))[0]\n",
    "\n",
    "    used_backend = \"PySR\"\n",
    "    save_results(\"PySR\", best[\"py\"], best[\"yhat_tr\"], best[\"yhat_te\"])\n",
    "\n",
    "except Exception:\n",
    "    # ---------- Fallback: gplearn (pure Python) ----------\n",
    "    from gplearn.genetic import SymbolicRegressor\n",
    "    from gplearn.functions import make_function\n",
    "\n",
    "    def _pdiv(x, y):\n",
    "        return np.divide(x, np.where(np.abs(y) < 1e-9, np.sign(y)*1e-9 + (y==0)*1e-9, y))\n",
    "    def _psqrt(x):\n",
    "        return np.sqrt(np.abs(x))\n",
    "    def _plog(x):\n",
    "        return np.log1p(np.abs(x))\n",
    "    def _pexp(x):\n",
    "        return np.exp(np.clip(x, -20, 20))\n",
    "\n",
    "    pdiv = make_function(function=_pdiv, name=\"pdiv\", arity=2)\n",
    "    psqrt = make_function(function=_psqrt, name=\"psqrt\", arity=1)\n",
    "    plog  = make_function(function=_plog,  name=\"plog\",  arity=1)\n",
    "    pexp  = make_function(function=_pexp,  name=\"pexp\",  arity=1)\n",
    "\n",
    "    gp = SymbolicRegressor(\n",
    "        function_set=(\"add\",\"sub\",\"mul\",pdiv,psqrt,plog,pexp),\n",
    "        metric=\"rmse\",\n",
    "        population_size=GP_POP_SIZE,\n",
    "        generations=GP_GENS,\n",
    "        tournament_size=GP_TOURN_SIZE,\n",
    "        const_range=(GP_CONST_MIN, GP_CONST_MAX),\n",
    "        init_depth=(2,6),\n",
    "        init_method=\"half and half\",\n",
    "        p_crossover=0.8,\n",
    "        p_subtree_mutation=0.01,\n",
    "        p_hoist_mutation=0.01,\n",
    "        p_point_mutation=0.08,\n",
    "        parsimony_coefficient=GP_PARSIMONY,\n",
    "        max_samples=0.9,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    gp.fit(X_tr, y_tr)\n",
    "\n",
    "    # Convert gplearn program string to a Python-evaluable expression with your feature names\n",
    "    def program_to_python(expr_str, var_names):\n",
    "        s = expr_str\n",
    "        for i, n in enumerate(var_names):\n",
    "            s = re.sub(rf\"\\bX{i}\\b\", n, s)\n",
    "        s = s.replace(\"add(\", \"ADD(\").replace(\"sub(\", \"SUB(\").replace(\"mul(\", \"MUL(\")\n",
    "        def bin_to_infix(text, token, op):\n",
    "            while token in text:\n",
    "                idx = text.find(token + \"(\")\n",
    "                if idx == -1: break\n",
    "                depth, j, comma = 0, idx + len(token) + 1, None\n",
    "                while j < len(text):\n",
    "                    if text[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif text[j] == \")\":\n",
    "                        if depth == 0: break\n",
    "                        depth -= 1\n",
    "                    elif text[j] == \",\" and depth == 0:\n",
    "                        comma = j; break\n",
    "                    j += 1\n",
    "                depth2, k = 0, comma + 1\n",
    "                while k < len(text):\n",
    "                    if text[k] == \"(\":\n",
    "                        depth2 += 1\n",
    "                    elif text[k] == \")\":\n",
    "                        if depth2 == 0: break\n",
    "                        depth2 -= 1\n",
    "                    k += 1\n",
    "                a = text[idx + len(token) + 1:comma]\n",
    "                b = text[comma + 1:k]\n",
    "                repl = \"(\" + a.strip() + f\" {op} \" + b.strip() + \")\"\n",
    "                text = text[:idx] + repl + text[k+1:]\n",
    "            return text\n",
    "        s = bin_to_infix(s, \"ADD\", \"+\")\n",
    "        s = bin_to_infix(s, \"SUB\", \"-\")\n",
    "        s = bin_to_infix(s, \"MUL\", \"*\")\n",
    "        return s\n",
    "\n",
    "    expr_str = str(gp._program)\n",
    "    eq_py = program_to_python(expr_str, FEATURES)\n",
    "\n",
    "    # Evaluate safely\n",
    "    def safe_eval_py(expr, Xmat, names):\n",
    "        env = {\n",
    "            \"np\": np,\n",
    "            \"pdiv\": lambda a,b: a/np.where(np.abs(b)<1e-9, np.sign(b)*1e-9 + (b==0)*1e-9, b),\n",
    "            \"psqrt\": lambda x: np.sqrt(np.abs(x)),\n",
    "            \"plog\":  lambda x: np.log1p(np.abs(x)),\n",
    "            \"pexp\":  lambda x: np.exp(np.clip(x, -20, 20)),\n",
    "        }\n",
    "        vals = {n: Xmat[:, i] for i, n in enumerate(names)}\n",
    "        return np.asarray(eval(expr, env, vals), float)\n",
    "\n",
    "    yhat_tr = safe_eval_py(eq_py, X_tr, FEATURES)\n",
    "    yhat_te = safe_eval_py(eq_py, X_te, FEATURES)\n",
    "\n",
    "    used_backend = \"gplearn\"\n",
    "    save_results(\"GP-Fallback\", eq_py, yhat_tr, yhat_te)\n",
    "\n",
    "print(f\"\\nArtifacts saved in: {os.path.abspath(OUTDIR)}\")\n",
    "print(\"Backend used:\", used_backend or \"unknown\")\n",
    "end = time.time()\n",
    "running_time = (end - start)\n",
    "print('Running Time: ', running_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36be204-72b7-426a-b328-1bdc3d25762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Controlling simplicity vs accuracy\n",
    "\n",
    "#Increase --max_evals / --niterations to search longer (find more accurate/possibly more complex formulas).\n",
    "\n",
    "#Lower --maxsize or raise parsimony (inside the script) to prefer simpler equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
