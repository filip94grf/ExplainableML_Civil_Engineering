{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69faf1-a3f5-4216-bcab-4e55769eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\III GODINA\\AIC M21 CASOPIS\\MATLAB CODE\\1.PRIPREMLJENA BAZA PODATAKA\\FUNDAMENTAL PERIOD PYTHON.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c9f741-41cd-45bf-92be-03ce3fe39e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great it works!\n",
    "\n",
    "# Can you now provide directly in the same way to jupyter-lab updated script for: \n",
    "# Script 6 — Genetic Programming Symbolic Regression (GEP-style)\n",
    "# Script 5 — LASSO Polynomial Regression (sparse closed-form)\n",
    "# Script 4 — Additive Spline GAM (explicit equation)\n",
    "# Script 3 — Model Tree (piecewise linear equations per region)\n",
    "# Script 2 — MARS (py-earth) to get piecewise-linear equations\n",
    "# Script 1 — Symbolic Regression (PySR) to discover equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821b86e8-9fcd-4ecf-aa1b-4f861cb98b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MARS-style — CFST ===\n",
      "KNOTS per feature (train): 4, quantile_range=(0.05, 0.95)\n",
      "Forward selected terms: 60 | Final after pruning: 40 | GCV=118917\n",
      "Train: {'R2': 0.985213, 'MAE': 181.587729, 'RMSE': 276.888014}\n",
      "Test : {'R2': 0.969235, 'MAE': 256.477503, 'RMSE': 417.069568}\n",
      "\n",
      "Closed-form (Python):\n",
      " y = +831.0187793733453 +19.45046240159981*max((B-101.436), 0) +0.3493143182739675*max((B-101.436), 0)*max((fc-28.8766), 0) -1.300784719132318*max((5.7592-t), 0)*max((74.4-fc), 0) -0.3323742087371549*max((B-101.436), 0)*max((28.8766-fc), 0) -129.0975447184525*max((B-199), 0) -12.05937495901545*max((101.436-B), 0) -0.01681779069342457*max((B-101.436), 0)*max((437.918-fy), 0) -0.7208005153277808*max((B-101.436), 0)*max((fc-74.4), 0) -0.3925299234864209*max((B-199), 0)*max((36.8-fc), 0) -0.006558642879666468*max((L-1200), 0)*max((fy-288.2), 0) -0.150239219923505*max((B-149.709), 0)*max((630-L), 0) -0.06207153772974447*max((149.709-B), 0)*max((fy-437.918), 0) +0.005039676793133698*max((L-630), 0)*max((fy-288.2), 0) +0.005018934544321163*max((630-L), 0)*max((fy-288.2), 0) -0.116670854562792*max((L-450), 0) -1.445938064367908*max((B-199), 0)*max((L-450), 0) +0.03119252197536553*max((B-149.709), 0)*max((L-1200), 0) +16.09450139775443*max((t-3), 0)*max((fc-74.4), 0) +0.8129574448696092*max((B-125), 0)*max((fc-74.4), 0) +8.679934398024441*max((B-101.436), 0)*max((t-4.8688), 0) +0.02678941289338654*max((125-B), 0)*max((630-L), 0) +1.481962937529149*max((B-199), 0)*max((L-368.16), 0) +0.4118937165358462*max((B-199), 0)*max((630-L), 0) -2.315730026854849*max((B-101.436), 0)*max((5.7592-t), 0) -16.96931298428847*max((t-3.84), 0)*max((fc-74.4), 0) -0.05236223910980992*max((B-199), 0)*max((L-1200), 0) -0.02323420158989603*max((B-149.709), 0)*max((L-630), 0) -4.603380344007571*max((4.8688-t), 0)*max((368.16-L), 0) +0.5529385713259715*max((t-3), 0)*max((fy-347.3), 0) +4.973717162726872*max((149.709-B), 0)*max((t-5.7592), 0) -3.936573700510613*max((t-5.7592), 0)*max((437.918-fy), 0) +6.389036993673852*max((t-5.7592), 0)*max((347.3-fy), 0) +0.0235960096861216*max((630-L), 0)*max((fc-36.8), 0) +1.76633315160317*max((fy-437.918), 0) +0.2328769338387874*max((B-149.709), 0)*max((450-L), 0) -0.07115361684391334*max((368.16-L), 0)*max((fc-47.4854), 0) -6.110402666134625*max((B-125), 0)*max((t-3.84), 0) +4.619398958335579*max((3.84-t), 0)*max((368.16-L), 0) +0.1393711738383737*max((B-149.709), 0)*max((74.4-fc), 0) +0.1700997428719646*max((B-199), 0)*max((288.2-fy), 0)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y = 4.61939895833558 \\max\\left(0, 3.84 - t\\right) \\max\\left(0, 368.16 - L\\right) - 4.60338034400757 \\max\\left(0, 4.8688 - t\\right) \\max\\left(0, 368.16 - L\\right) - 1.30078471913232 \\max\\left(0, 5.7592 - t\\right) \\max\\left(0, 74.4 - fc\\right) - 2.31573002685485 \\max\\left(0, 5.7592 - t\\right) \\max\\left(0, B - 101.436\\right) - 0.332374208737155 \\max\\left(0, 28.8766 - fc\\right) \\max\\left(0, B - 101.436\\right) - 0.392529923486421 \\max\\left(0, 36.8 - fc\\right) \\max\\left(0, B - 199\\right) + 0.139371173838374 \\max\\left(0, 74.4 - fc\\right) \\max\\left(0, B - 149.709\\right) - 12.0593749590155 \\max\\left(0, 101.436 - B\\right) + 0.0267894128933865 \\max\\left(0, 125 - B\\right) \\max\\left(0, 630 - L\\right) - 0.0620715377297445 \\max\\left(0, 149.709 - B\\right) \\max\\left(0, fy - 437.918\\right) + 4.97371716272687 \\max\\left(0, 149.709 - B\\right) \\max\\left(0, t - 5.7592\\right) + 0.170099742871965 \\max\\left(0, 288.2 - fy\\right) \\max\\left(0, B - 199\\right) + 6.38903699367385 \\max\\left(0, 347.3 - fy\\right) \\max\\left(0, t - 5.7592\\right) - 0.0711536168439133 \\max\\left(0, 368.16 - L\\right) \\max\\left(0, fc - 47.4854\\right) - 0.0168177906934246 \\max\\left(0, 437.918 - fy\\right) \\max\\left(0, B - 101.436\\right) - 3.93657370051061 \\max\\left(0, 437.918 - fy\\right) \\max\\left(0, t - 5.7592\\right) + 0.232876933838787 \\max\\left(0, 450 - L\\right) \\max\\left(0, B - 149.709\\right) + 0.411893716535846 \\max\\left(0, 630 - L\\right) \\max\\left(0, B - 199\\right) - 0.150239219923505 \\max\\left(0, 630 - L\\right) \\max\\left(0, B - 149.709\\right) + 0.0235960096861216 \\max\\left(0, 630 - L\\right) \\max\\left(0, fc - 36.8\\right) + 0.00501893454432116 \\max\\left(0, 630 - L\\right) \\max\\left(0, fy - 288.2\\right) - 0.0523622391098099 \\max\\left(0, B - 199\\right) \\max\\left(0, L - 1200\\right) - 1.44593806436791 \\max\\left(0, B - 199\\right) \\max\\left(0, L - 450\\right) + 1.48196293752915 \\max\\left(0, B - 199\\right) \\max\\left(0, L - 368.16\\right) - 129.097544718453 \\max\\left(0, B - 199\\right) + 0.0311925219753655 \\max\\left(0, B - 149.709\\right) \\max\\left(0, L - 1200\\right) - 0.023234201589896 \\max\\left(0, B - 149.709\\right) \\max\\left(0, L - 630\\right) + 0.812957444869609 \\max\\left(0, B - 125\\right) \\max\\left(0, fc - 74.4\\right) - 6.11040266613463 \\max\\left(0, B - 125\\right) \\max\\left(0, t - 3.84\\right) - 0.720800515327781 \\max\\left(0, B - 101.436\\right) \\max\\left(0, fc - 74.4\\right) + 0.349314318273968 \\max\\left(0, B - 101.436\\right) \\max\\left(0, fc - 28.8766\\right) + 8.67993439802444 \\max\\left(0, B - 101.436\\right) \\max\\left(0, t - 4.8688\\right) + 19.4504624015998 \\max\\left(0, B - 101.436\\right) - 0.00655864287966647 \\max\\left(0, L - 1200\\right) \\max\\left(0, fy - 288.2\\right) + 0.0050396767931337 \\max\\left(0, L - 630\\right) \\max\\left(0, fy - 288.2\\right) - 0.116670854562792 \\max\\left(0, L - 450\\right) - 16.9693129842885 \\max\\left(0, fc - 74.4\\right) \\max\\left(0, t - 3.84\\right) + 16.0945013977544 \\max\\left(0, fc - 74.4\\right) \\max\\left(0, t - 3\\right) + 1.76633315160317 \\max\\left(0, fy - 437.918\\right) + 0.552938571325971 \\max\\left(0, fy - 347.3\\right) \\max\\left(0, t - 3\\right) + 831.018779373345$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time:  52.43389081954956  seconds\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER CELL — Script 2: MARS-style (degree=2 with pairwise products), pure Python\n",
    "# Dataset: ['NoSt','NoSp','LoSp','OP','MWS'] -> TFP (original units)\n",
    "# Greedy forward selection + backward pruning using hinge and hinge*hinge bases.\n",
    "# Displays pretty math, prints metrics, and saves small artifacts.\n",
    "\n",
    "import os, json, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start = time.time()\n",
    "# ---------- CONFIG ----------\n",
    "EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\IIIII GODINA\\8.CSP - NOVA KNJIGA SA VM\\MOJE POGLAVLJE\\CASE STUDIES\\CFST columns Dataset.xlsx\"\n",
    "SHEET      = 0\n",
    "FEATURES = [\"B\",\"t\",\"L\",\"fy\",\"fc\"]\n",
    "TARGET = \"Nexp\"\n",
    "\n",
    "TEST_SIZE   = 0.20\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Hinge candidate generation on TRAIN ONLY (then reused for TEST)\n",
    "# Make this smaller if runtime/memory is a concern; larger gives more flexibility.\n",
    "N_KNOTS_PER_FEATURE = 4            # interior knots per feature\n",
    "QUANTILE_RANGE      = (0.05, 0.95) # ignore tails\n",
    "\n",
    "# Forward / backward settings\n",
    "MAX_TERMS   = 60        # max number of basis terms (excluding intercept) to add in forward step\n",
    "MIN_IMPROV  = 1e-6      # stop forward when best RSS improvement below this\n",
    "PENALTY_GCV = 4.0       # GCV penalty (higher => simpler after pruning)\n",
    "\n",
    "OUTDIR = \"out_mars_style_cfst\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "missing = [c for c in FEATURES + [TARGET] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\\nPresent: {list(df.columns)}\")\n",
    "\n",
    "X = df[FEATURES].apply(pd.to_numeric, errors=\"coerce\").values\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\").values\n",
    "mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "n_tr, p = X_tr.shape\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        RMSE=math.sqrt(((y_true - y_pred)**2).mean())\n",
    "    )\n",
    "\n",
    "def ols_fit(Z, y):\n",
    "    \"\"\"OLS with intercept; returns beta (incl. intercept), yhat and RSS.\"\"\"\n",
    "    Z1 = np.column_stack([np.ones(len(Z)), Z]) if Z.size else np.ones((len(y), 1))\n",
    "    beta, *_ = np.linalg.lstsq(Z1, y, rcond=None)\n",
    "    yhat = Z1 @ beta\n",
    "    rss = float(((y - yhat) ** 2).sum())\n",
    "    return beta, yhat, rss\n",
    "\n",
    "def gcv(rss, n, k, penalty=2.0):\n",
    "    \"\"\"MARS-like GCV: denom = (1 - (C/n))^2, with C = 1 + penalty*k (1 = intercept).\"\"\"\n",
    "    C = 1.0 + penalty * k\n",
    "    denom = (1.0 - C / n)\n",
    "    denom = max(denom, 1e-12)\n",
    "    return rss / (n * denom * denom)\n",
    "\n",
    "# ---------- TRAIN knots (for single hinges), then reuse ----------\n",
    "def compute_knots_train(Xtr, n_knots=10, qrange=(0.05, 0.95)):\n",
    "    knots_by_feature = []\n",
    "    q_lo, q_hi = qrange\n",
    "    qs = np.linspace(q_lo, q_hi, n_knots + 2)[1:-1]  # interior only\n",
    "    for j in range(Xtr.shape[1]):\n",
    "        x = Xtr[:, j]\n",
    "        ks = np.quantile(x, qs)\n",
    "        ks = np.unique(ks.round(12))\n",
    "        knots_by_feature.append(ks)\n",
    "    return knots_by_feature\n",
    "\n",
    "def build_single_hinges(Xmat, feature_names, knots_by_feature):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        Z_singles: (n x M1) matrix of single hinges\n",
    "        names_s:  list of 'h(<expr>)' strings\n",
    "        meta_s:   list of dicts with {'feat': j, 'knot': t, 'dir': +1/-1}\n",
    "    \"\"\"\n",
    "    Z_cols, names, meta = [], [], []\n",
    "    for j, name in enumerate(feature_names):\n",
    "        x = Xmat[:, j]\n",
    "        for t in knots_by_feature[j]:\n",
    "            Z_cols.append(np.maximum(0.0, x - t))\n",
    "            names.append(f\"h({name}-{t:.6g})\")\n",
    "            meta.append({\"feat\": j, \"knot\": float(t), \"dir\": +1})\n",
    "            Z_cols.append(np.maximum(0.0, t - x))\n",
    "            names.append(f\"h({t:.6g}-{name})\")\n",
    "            meta.append({\"feat\": j, \"knot\": float(t), \"dir\": -1})\n",
    "    if not Z_cols:\n",
    "        return np.empty((len(Xmat), 0)), [], []\n",
    "    return np.column_stack(Z_cols), names, meta\n",
    "\n",
    "def build_pair_hinges(Xmat, names_s, meta_s, idx_pairs):\n",
    "    \"\"\"\n",
    "    Multiply selected pairs of single hinges to form degree-2 interactions.\n",
    "    idx_pairs: list of (i, j) indices into the single-hinge basis.\n",
    "    Returns (Z_pairs, names_pairs).\n",
    "    \"\"\"\n",
    "    if not idx_pairs:\n",
    "        return np.empty((Xmat.shape[0], 0)), []\n",
    "    Z_cols, names = [], []\n",
    "    # We will regenerate the single hinges for Xmat from the names_s only when needed.\n",
    "    # But to be efficient, pass the already-built single hinges if available.\n",
    "    # For simplicity here, we compute them by evaluating the name string.\n",
    "    # However, we already have X_tr singles prebuilt; we'll use those for train,\n",
    "    # and rebuild for test using stored knots/meta.\n",
    "    raise_if = False  # placeholder to note that we shouldn't be here; we won't call this version.\n",
    "\n",
    "# Build TRAIN single hinges and their metadata\n",
    "knots_train = compute_knots_train(X_tr, N_KNOTS_PER_FEATURE, QUANTILE_RANGE)\n",
    "Zs_tr, names_s, meta_s = build_single_hinges(X_tr, FEATURES, knots_train)\n",
    "\n",
    "# Build TEST single hinges using the SAME knots\n",
    "def build_single_hinges_from_meta(Xmat, feature_names, meta_list):\n",
    "    Z_cols, names = [], []\n",
    "    for m in meta_list:\n",
    "        j, t, d = m[\"feat\"], m[\"knot\"], m[\"dir\"]\n",
    "        x = Xmat[:, j]\n",
    "        if d == +1:\n",
    "            Z_cols.append(np.maximum(0.0, x - t)); names.append(f\"h({feature_names[j]}-{t:.6g})\")\n",
    "        else:\n",
    "            Z_cols.append(np.maximum(0.0, t - x)); names.append(f\"h({t:.6g}-{feature_names[j]})\")\n",
    "    return np.column_stack(Z_cols), names\n",
    "\n",
    "Zs_te, _ = build_single_hinges_from_meta(X_te, FEATURES, meta_s)\n",
    "\n",
    "# Build PAIR indices across different features (degree=2)\n",
    "# To keep scale reasonable, we build all cross-feature pairs:\n",
    "single_count = len(names_s)\n",
    "feat_of = np.array([m[\"feat\"] for m in meta_s], dtype=int)\n",
    "pair_indices = []\n",
    "for i in range(single_count):\n",
    "    fi = feat_of[i]\n",
    "    for j in range(i+1, single_count):\n",
    "        fj = feat_of[j]\n",
    "        if fi != fj:  # cross-feature only (MARS degree=2)\n",
    "            pair_indices.append((i, j))\n",
    "\n",
    "# Construct TRAIN pair matrix (hinge_i * hinge_j)\n",
    "def multiply_pairs(Z_single, pair_idx_list):\n",
    "    if not pair_idx_list:\n",
    "        return np.empty((Z_single.shape[0], 0))\n",
    "    n = Z_single.shape[0]\n",
    "    Zp = np.empty((n, len(pair_idx_list)), dtype=float)\n",
    "    for k, (i, j) in enumerate(pair_idx_list):\n",
    "        Zp[:, k] = Z_single[:, i] * Z_single[:, j]\n",
    "    return Zp\n",
    "\n",
    "Zp_tr = multiply_pairs(Zs_tr, pair_indices)\n",
    "\n",
    "# Pair names for reporting\n",
    "names_p = [f\"{names_s[i]}*{names_s[j]}\" for (i, j) in pair_indices]\n",
    "\n",
    "# Build TEST pair matrix using TEST singles (same pairing structure)\n",
    "Zp_te = multiply_pairs(Zs_te, pair_indices)\n",
    "\n",
    "# Combine SINGLE + PAIR into full design\n",
    "Z_tr_full = np.column_stack([Zs_tr, Zp_tr])\n",
    "Z_te_full = np.column_stack([Zs_te, Zp_te])\n",
    "names_full = names_s + names_p\n",
    "\n",
    "# ---------- Forward selection ----------\n",
    "selected_idx = []\n",
    "current_Z = np.empty((n_tr, 0))\n",
    "beta, yhat, rss = ols_fit(current_Z, y_tr)\n",
    "fw_trace = [(selected_idx.copy(), beta.copy(), rss)]\n",
    "\n",
    "for step in range(MAX_TERMS):\n",
    "    best_improve = 0.0\n",
    "    best_j = None\n",
    "    # Try adding each unused candidate\n",
    "    for j in range(Z_tr_full.shape[1]):\n",
    "        if j in selected_idx:\n",
    "            continue\n",
    "        Z_try = np.column_stack([current_Z, Z_tr_full[:, j]])\n",
    "        _, _, rss_try = ols_fit(Z_try, y_tr)\n",
    "        improve = rss - rss_try\n",
    "        if improve > best_improve:\n",
    "            best_improve = improve\n",
    "            best_j = j\n",
    "    if best_j is None or best_improve < MIN_IMPROV:\n",
    "        break\n",
    "    selected_idx.append(best_j)\n",
    "    current_Z = np.column_stack([current_Z, Z_tr_full[:, best_j]])\n",
    "    beta, yhat, rss = ols_fit(current_Z, y_tr)\n",
    "    fw_trace.append((selected_idx.copy(), beta.copy(), rss))\n",
    "\n",
    "# ---------- Backward pruning (GCV) ----------\n",
    "def prune_backward(Z_full, y, selected):\n",
    "    snapshots = []\n",
    "    sel = selected.copy()\n",
    "    Z_sel = Z_full[:, sel] if sel else np.empty((len(y),0))\n",
    "    beta, _, rss = ols_fit(Z_sel, y)\n",
    "    k = len(sel)\n",
    "    snapshots.append((sel.copy(), beta.copy(), rss, gcv(rss, len(y), k, PENALTY_GCV)))\n",
    "\n",
    "    while len(sel) > 0:\n",
    "        best_gcv = float(\"inf\")\n",
    "        best_drop = None\n",
    "        best_beta = None\n",
    "        best_rss = None\n",
    "        for idx_pos in range(len(sel)):\n",
    "            trial = sel[:idx_pos] + sel[idx_pos+1:]\n",
    "            Z_trial = Z_full[:, trial] if trial else np.empty((len(y),0))\n",
    "            beta_t, _, rss_t = ols_fit(Z_trial, y)\n",
    "            gcv_t = gcv(rss_t, len(y), len(trial), PENALTY_GCV)\n",
    "            if gcv_t < best_gcv:\n",
    "                best_gcv = gcv_t\n",
    "                best_drop = idx_pos\n",
    "                best_beta = beta_t\n",
    "                best_rss = rss_t\n",
    "        sel.pop(best_drop)\n",
    "        snapshots.append((sel.copy(), best_beta.copy(), best_rss, best_gcv))\n",
    "    # choose snapshot with min GCV\n",
    "    return min(snapshots, key=lambda tup: tup[3])\n",
    "\n",
    "sel_fw, beta_fw, rss_fw = fw_trace[-1]\n",
    "sel_final, beta_final, rss_final, gcv_final = prune_backward(Z_tr_full, y_tr, sel_fw)\n",
    "\n",
    "# ---------- Predictions & metrics ----------\n",
    "def predict_from_indices(Z_full, beta, sel_idx):\n",
    "    if len(sel_idx) == 0:\n",
    "        Z1 = np.ones((Z_full.shape[0], 1))\n",
    "    else:\n",
    "        Z1 = np.column_stack([np.ones(Z_full.shape[0]), Z_full[:, sel_idx]])\n",
    "    return Z1 @ beta\n",
    "\n",
    "yhat_tr = predict_from_indices(Z_tr_full, beta_final, sel_final)\n",
    "yhat_te = predict_from_indices(Z_te_full, beta_final, sel_final)\n",
    "\n",
    "m_train = metrics(y_tr, yhat_tr)\n",
    "m_test  = metrics(y_te, yhat_te)\n",
    "\n",
    "# ---------- Pretty equation (SymPy) ----------\n",
    "from sympy import symbols, Max, sympify, latex\n",
    "from IPython.display import Math, display\n",
    "\n",
    "sym_vars = {name: symbols(name, real=True) for name in FEATURES}\n",
    "\n",
    "def hinge_name_to_sympy(nm: str):\n",
    "    inside = nm[nm.find(\"(\")+1: nm.rfind(\")\")]  # e.g., \"NoSt-3.2\" or \"3.2-NoSt\"\n",
    "    return Max(sympify(inside, locals=sym_vars), 0)\n",
    "\n",
    "# Decode whether a selected term is single or pair\n",
    "single_len = len(names_s)\n",
    "terms_report = [{\"term\": \"1 (intercept)\", \"coefficient\": float(beta_final[0])}]\n",
    "sym_expr = sympify(float(beta_final[0]))\n",
    "\n",
    "for pos, j in enumerate(sel_final, start=1):\n",
    "    nm = names_full[j]\n",
    "    c  = float(beta_final[pos])\n",
    "    if j < single_len:  # single hinge\n",
    "        term_expr = hinge_name_to_sympy(nm)\n",
    "    else:               # pair: \"h(..)*h(..)\"\n",
    "        left, right = nm.split(\"*\", 1)\n",
    "        term_expr = hinge_name_to_sympy(left) * hinge_name_to_sympy(right)\n",
    "    sym_expr += c * term_expr\n",
    "    terms_report.append({\"term\": nm, \"coefficient\": c})\n",
    "\n",
    "def python_equation(beta, sel_idx, names):\n",
    "    parts = [f\"{float(beta[0]):+.16g}\"]\n",
    "    for pos, j in enumerate(sel_idx, start=1):\n",
    "        nm = names[j]\n",
    "        if \"*\" in nm:\n",
    "            lft, rgt = nm.split(\"*\", 1)\n",
    "            in_l = lft[lft.find(\"(\")+1: lft.rfind(\")\")]\n",
    "            in_r = rgt[rgt.find(\"(\")+1: rgt.rfind(\")\")]\n",
    "            parts.append(f\"{float(beta[pos]):+.16g}*max(({in_l}), 0)*max(({in_r}), 0)\")\n",
    "        else:\n",
    "            inside = nm[nm.find(\"(\")+1: nm.rfind(\")\")]\n",
    "            parts.append(f\"{float(beta[pos]):+.16g}*max(({inside}), 0)\")\n",
    "    return \"y = \" + \" \".join(parts)\n",
    "\n",
    "eq_python = python_equation(beta_final, sel_final, names_full)\n",
    "\n",
    "# ---------- OUTPUT ----------\n",
    "print(\"=== MARS-style — CFST ===\")\n",
    "print(f\"KNOTS per feature (train): {N_KNOTS_PER_FEATURE}, quantile_range={QUANTILE_RANGE}\")\n",
    "print(f\"Forward selected terms: {len(sel_fw)} | Final after pruning: {len(sel_final)} | GCV={gcv_final:.6g}\")\n",
    "print(\"Train:\", {k: round(v, 6) for k, v in m_train.items()})\n",
    "print(\"Test :\", {k: round(v, 6) for k, v in m_test.items()})\n",
    "print(\"\\nClosed-form (Python):\\n\", eq_python)\n",
    "\n",
    "# Pretty math in notebook\n",
    "display(Math(r\"y = \" + latex(sym_expr)))\n",
    "\n",
    "# ---------- Save artifacts ----------\n",
    "with open(os.path.join(OUTDIR, \"equation_python.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(eq_python + \"\\n\")\n",
    "pd.DataFrame(terms_report).to_csv(os.path.join(OUTDIR, \"terms_selected.csv\"), index=False)\n",
    "with open(os.path.join(OUTDIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"file\": EXCEL_PATH,\n",
    "        \"sheet\": SHEET,\n",
    "        \"features\": FEATURES,\n",
    "        \"target\": TARGET,\n",
    "        \"degree\": 2,\n",
    "        \"n_knots_per_feature\": N_KNOTS_PER_FEATURE,\n",
    "        \"quantile_range\": QUANTILE_RANGE,\n",
    "        \"forward_max_terms\": MAX_TERMS,\n",
    "        \"penalty_gcv\": PENALTY_GCV,\n",
    "        \"forward_selected\": len(sel_fw),\n",
    "        \"final_selected\": len(sel_final),\n",
    "        \"train\": m_train,\n",
    "        \"test\":  m_test,\n",
    "        \"gcv_final\": gcv_final\n",
    "    }, f, indent=2)\n",
    "\n",
    "end = time.time()\n",
    "running_time = (end - start)\n",
    "print('Running Time: ', running_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36be204-72b7-426a-b328-1bdc3d25762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity controls:\n",
    "\n",
    "# N_KNOTS_PER_FEATURE & QUANTILE_RANGE → candidate richness.\n",
    "\n",
    "# MAX_TERMS & MIN_IMPROV → forward stopping.\n",
    "\n",
    "# PENALTY_GCV → how aggressively the pruning simplifies the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc952fd-5458-4eb5-936d-4ea230f769d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune simplicity vs. accuracy:\n",
    "\n",
    "# Make it simpler: increase --penalty (e.g., 3–5) and/or reduce --max_terms, set --max_degree 1 (additive only).\n",
    "\n",
    "# Make it more accurate: lower --penalty, allow --max_degree 2, raise --max_terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
