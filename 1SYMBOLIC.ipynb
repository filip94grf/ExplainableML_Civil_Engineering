{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69faf1-a3f5-4216-bcab-4e55769eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\III GODINA\\AIC M21 CASOPIS\\MATLAB CODE\\1.PRIPREMLJENA BAZA PODATAKA\\FUNDAMENTAL PERIOD PYTHON.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c9f741-41cd-45bf-92be-03ce3fe39e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '—' (U+2014) (3403033769.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    Script 4 — Additive Spline GAM (explicit equation)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '—' (U+2014)\n"
     ]
    }
   ],
   "source": [
    "# Great it works!\n",
    "\n",
    "# Can you now provide directly in the same way to jupyter-lab updated script for: \n",
    "# Script 6 — Genetic Programming Symbolic Regression (GEP-style)\n",
    "# Script 5 — LASSO Polynomial Regression (sparse closed-form)\n",
    "# Script 4 — Additive Spline GAM (explicit equation)\n",
    "# Script 3 — Model Tree (piecewise linear equations per region)\n",
    "# Script 2 — MARS (py-earth) to get piecewise-linear equations\n",
    "# Script 1 — Symbolic Regression (PySR) to discover equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821b86e8-9fcd-4ecf-aa1b-4f861cb98b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Julia backend...\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    12.94      2.88224e+24       16         0.527362         0.503087      1.52m\n",
      "   1     7.15      2.49127e+07       16         0.522699         0.545126      1.32m\n",
      "   2     7.87      1.99066e+07       13         0.433478         0.421782      1.53m\n",
      "   3    11.46      4.83744e+07       13          0.42794         0.469934      1.48m\n",
      "   4    15.95      5.13517e+07       19         0.377906         0.414835      2.13m\n",
      "   5    17.25      9.17656e+07       20         0.366054         0.349769      1.65m\n",
      "   6    20.45      6.48924e+07       30         0.320734         0.316857      1.99m\n",
      "   7    23.14      3.47934e+07       30         0.317944         0.341224      1.91m\n",
      "   8    24.30      2.46698e+07       24         0.287541         0.268299      2.36m\n",
      "   9    24.65       5.9825e+06       26         0.276026         0.270139      2.10m\n",
      "  10    27.92      1.20598e+07       26         0.275785         0.272346      2.13m\n",
      "  11    30.75      1.46211e+06       58          0.25629         0.240885      3.07m\n",
      "  12    30.78      5.47847e+06       29          0.25098         0.261605      2.55m\n",
      "  13    29.26      2.01226e+07       29         0.250497          0.26574      1.75m\n",
      "  14    25.85          80527.4       30         0.250798         0.282619      1.90m\n",
      "  15    25.19           106161       37         0.245996         0.251601      1.58m\n",
      "  16    27.17          37134.8       32         0.232653         0.225375      1.64m\n",
      "  17    27.91           263709       32         0.230477         0.244678      1.33m\n",
      "  18    26.68      4.54959e+06       32         0.230729         0.242527      1.40m\n",
      "  19    25.10           238416       34         0.229367         0.209912     59.04s\n",
      "  20    24.35          64188.2       39           0.2189         0.221976     59.65s\n",
      "  21    26.08           927208       39         0.214985         0.254091     55.47s\n",
      "  22    29.22          5370.22       39         0.217637         0.232892     50.81s\n",
      "  23    30.31          31882.5       36         0.215443         0.222411     46.04s\n",
      "  24    30.01          25416.4       68         0.211243         0.213809     41.49s\n",
      "  25    28.49      1.74011e+11       36         0.214816         0.227801     25.30s\n",
      "  26    27.51           117838       59         0.209711         0.217747     21.35s\n",
      "  27    26.07           321305       77         0.214768         0.202577     12.25s\n",
      "  28    26.01           996570       28         0.219719         0.248508      6.17s\n",
      "  29    25.92           361177       27         0.220214         0.250641      0.00s\n",
      "\n",
      "=== GP-Fallback — Symbolic Regression (original units) ===\n",
      "Train: {'R2': 0.917483, 'MAE': 0.150428, 'RMSE': 0.22347}\n",
      "Test : {'R2': 0.929224, 'MAE': 0.141364, 'RMSE': 0.216125}\n",
      "\n",
      "Closed-form (Python):\n",
      " y = plog(((pdiv(plog((-1.056 + (plog(plog(plog(plog(0.425)))) * OP))), pdiv(pexp(4.778), pexp(-1.006))) * LoSp) * (pexp(psqrt(NoSt)) + pexp(psqrt(NoSt)))))\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y = \\log{\\left(0.00615277046363411 e^{\\sqrt{\\left|{NoSt}\\right|}} \\log{\\left(\\left|{0.234925985657972 OP - 1.056}\\right| + 1 \\right)} \\left|{LoSp}\\right| + 1 \\right)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts saved in: C:\\Users\\filip\\Documents\\POGLAVLJE KNJIGE\\out_symbolic_period\n",
      "Backend used: gplearn\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER CELL — Script 1: Symbolic Regression with pretty math display\n",
    "# Dataset: X = ['NoSt','NoSp','LoSp','OP','MWS'], y = 'TFP' (original units)\n",
    "\n",
    "import os, math, json, warnings, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "EXCEL_PATH = r\"D:\\FILIP\\DOKTORSKE STUDIJE\\III GODINA\\AIC M21 CASOPIS\\MATLAB CODE\\1.PRIPREMLJENA BAZA PODATAKA\\FUNDAMENTAL PERIOD PYTHON.xlsx\"\n",
    "SHEET = 0\n",
    "FEATURES = [\"NoSt\",\"NoSp\",\"LoSp\",\"OP\",\"MWS\"]\n",
    "TARGET = \"TFP\"\n",
    "\n",
    "TEST_SIZE   = 0.20\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# PySR knobs (used only if PySR+Julia available)\n",
    "PYSR_NITER     = 50\n",
    "PYSR_MAXSIZE   = 30\n",
    "PYSR_MAX_EVALS = 800\n",
    "PYSR_BIN_OPS   = [\"+\", \"-\", \"*\", \"/\", \"^\"]\n",
    "PYSR_UN_OPS    = [\"sqrt\"]\n",
    "PYSR_CONSTRAINTS = {\"^\": (5, 1)}\n",
    "\n",
    "# gplearn (fallback) knobs\n",
    "GP_POP_SIZE   = 3000\n",
    "GP_GENS       = 30\n",
    "GP_TOURN_SIZE = 20\n",
    "GP_PARSIMONY  = 0.001\n",
    "GP_CONST_MIN, GP_CONST_MAX = -5.0, 5.0\n",
    "\n",
    "OUTDIR = \"out_symbolic_period\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "missing = [c for c in FEATURES + [TARGET] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in Excel: {missing}\\nPresent: {list(df.columns)}\")\n",
    "\n",
    "X = df[FEATURES].apply(pd.to_numeric, errors=\"coerce\").values\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\").values\n",
    "mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        RMSE=math.sqrt(((y_true - y_pred)**2).mean()),\n",
    "    )\n",
    "\n",
    "# ---------- Pretty math display (SymPy) ----------\n",
    "from sympy import sympify, Abs, sqrt, log, exp, latex, simplify, symbols\n",
    "from IPython.display import Math, display\n",
    "\n",
    "def show_math_equation(eq_py: str, var_order):\n",
    "    \"\"\"\n",
    "    Render 'y = <expr>' in proper math notation using SymPy.\n",
    "    Maps:\n",
    "      pdiv(a,b) -> a/(b + 1e-9)\n",
    "      psqrt(x)  -> sqrt(|x|)\n",
    "      plog(x)   -> log(1 + |x|)\n",
    "      pexp(x)   -> exp(x)\n",
    "    \"\"\"\n",
    "    # Create SymPy symbols for variables\n",
    "    syms = symbols(\" \".join(var_order), real=True)\n",
    "    loc = {name: sym for name, sym in zip(var_order, syms)}\n",
    "    # Map protected functions to SymPy expressions\n",
    "    loc.update({\n",
    "        \"pdiv\": lambda a, b: a/(b + 1e-9),\n",
    "        \"psqrt\": lambda x: sqrt(Abs(x)),\n",
    "        \"plog\": lambda x: log(1 + Abs(x)),\n",
    "        \"pexp\": lambda x: exp(x),\n",
    "    })\n",
    "    try:\n",
    "        expr = sympify(eq_py, locals=loc)\n",
    "        expr = simplify(expr)\n",
    "        display(Math(r\"y = \" + latex(expr)))\n",
    "    except Exception as e:\n",
    "        print(\"Could not render pretty math; showing raw expression instead.\")\n",
    "        print(\"y =\", eq_py)\n",
    "\n",
    "def save_results(name, eq_py, yhat_tr, yhat_te):\n",
    "    mt = metrics(y_te, yhat_te); mtr = metrics(y_tr, yhat_tr)\n",
    "    print(f\"\\n=== {name} — Symbolic Regression (original units) ===\")\n",
    "    print(\"Train:\", {k: round(v, 6) for k, v in mtr.items()})\n",
    "    print(\"Test :\", {k: round(v, 6) for k, v in mt.items()})\n",
    "    print(\"\\nClosed-form (Python):\\n\", \"y = \" + eq_py)\n",
    "    # Pretty math:\n",
    "    show_math_equation(eq_py, FEATURES)\n",
    "\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_equation_python.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"y = \" + eq_py + \"\\n\")\n",
    "    with open(os.path.join(OUTDIR, f\"{name.lower()}_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train\": mtr, \"test\": mt}, f, indent=2)\n",
    "\n",
    "used_backend = None\n",
    "\n",
    "# ---------- Try PySR first ----------\n",
    "try:\n",
    "    from pysr import PySRRegressor\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        niterations=PYSR_NITER,\n",
    "        maxsize=PYSR_MAXSIZE,\n",
    "        populations=30,\n",
    "        population_size=60,\n",
    "        parsimony=1e-4,\n",
    "        progress=True,\n",
    "        binary_operators=PYSR_BIN_OPS,\n",
    "        unary_operators=PYSR_UN_OPS,\n",
    "        loss=\"L2DistLoss()\",\n",
    "        model_selection=\"best\",\n",
    "        max_evals=PYSR_MAX_EVALS,\n",
    "        random_state=RANDOM_SEED,\n",
    "        constraints=PYSR_CONSTRAINTS,\n",
    "    ).fit(X_tr, y_tr, variable_names=FEATURES)\n",
    "\n",
    "    # Pick simplest within 5% of best test MAE\n",
    "    candidates = []\n",
    "    for _, row in model.equations_.iterrows():\n",
    "        # 'equation' is a Python-evaluable string produced by PySR (in terms of variable names)\n",
    "        eq_py = row[\"equation\"]\n",
    "        # Evaluate this candidate\n",
    "        f = model.lambda_format()(row[\"equation\"])\n",
    "        yhat_tr = np.array(f(X_tr), float)\n",
    "        yhat_te = np.array(f(X_te), float)\n",
    "        candidates.append({\n",
    "            \"complexity\": int(row[\"complexity\"]),\n",
    "            \"py\": eq_py,\n",
    "            \"train\": metrics(y_tr, yhat_tr),\n",
    "            \"test\":  metrics(y_te, yhat_te),\n",
    "            \"yhat_tr\": yhat_tr, \"yhat_te\": yhat_te\n",
    "        })\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"PySR returned no parsable equations.\")\n",
    "\n",
    "    best_mae = min(c[\"test\"][\"MAE\"] for c in candidates)\n",
    "    pool = [c for c in candidates if c[\"test\"][\"MAE\"] <= 1.05*best_mae]\n",
    "    best = sorted(pool, key=lambda c: (c[\"complexity\"], c[\"test\"][\"MAE\"]))[0]\n",
    "\n",
    "    used_backend = \"PySR\"\n",
    "    save_results(\"PySR\", best[\"py\"], best[\"yhat_tr\"], best[\"yhat_te\"])\n",
    "\n",
    "except Exception:\n",
    "    # ---------- Fallback: gplearn (pure Python) ----------\n",
    "    from gplearn.genetic import SymbolicRegressor\n",
    "    from gplearn.functions import make_function\n",
    "\n",
    "    def _pdiv(x, y):\n",
    "        return np.divide(x, np.where(np.abs(y) < 1e-9, np.sign(y)*1e-9 + (y==0)*1e-9, y))\n",
    "    def _psqrt(x):\n",
    "        return np.sqrt(np.abs(x))\n",
    "    def _plog(x):\n",
    "        return np.log1p(np.abs(x))\n",
    "    def _pexp(x):\n",
    "        return np.exp(np.clip(x, -20, 20))\n",
    "\n",
    "    pdiv = make_function(function=_pdiv, name=\"pdiv\", arity=2)\n",
    "    psqrt = make_function(function=_psqrt, name=\"psqrt\", arity=1)\n",
    "    plog  = make_function(function=_plog,  name=\"plog\",  arity=1)\n",
    "    pexp  = make_function(function=_pexp,  name=\"pexp\",  arity=1)\n",
    "\n",
    "    gp = SymbolicRegressor(\n",
    "        function_set=(\"add\",\"sub\",\"mul\",pdiv,psqrt,plog,pexp),\n",
    "        metric=\"rmse\",\n",
    "        population_size=GP_POP_SIZE,\n",
    "        generations=GP_GENS,\n",
    "        tournament_size=GP_TOURN_SIZE,\n",
    "        const_range=(GP_CONST_MIN, GP_CONST_MAX),\n",
    "        init_depth=(2,6),\n",
    "        init_method=\"half and half\",\n",
    "        p_crossover=0.8,\n",
    "        p_subtree_mutation=0.01,\n",
    "        p_hoist_mutation=0.01,\n",
    "        p_point_mutation=0.08,\n",
    "        parsimony_coefficient=GP_PARSIMONY,\n",
    "        max_samples=0.9,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    gp.fit(X_tr, y_tr)\n",
    "\n",
    "    # Convert gplearn program string to a Python-evaluable expression with your feature names\n",
    "    def program_to_python(expr_str, var_names):\n",
    "        s = expr_str\n",
    "        for i, n in enumerate(var_names):\n",
    "            s = re.sub(rf\"\\bX{i}\\b\", n, s)\n",
    "        s = s.replace(\"add(\", \"ADD(\").replace(\"sub(\", \"SUB(\").replace(\"mul(\", \"MUL(\")\n",
    "        def bin_to_infix(text, token, op):\n",
    "            while token in text:\n",
    "                idx = text.find(token + \"(\")\n",
    "                if idx == -1: break\n",
    "                depth, j, comma = 0, idx + len(token) + 1, None\n",
    "                while j < len(text):\n",
    "                    if text[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif text[j] == \")\":\n",
    "                        if depth == 0: break\n",
    "                        depth -= 1\n",
    "                    elif text[j] == \",\" and depth == 0:\n",
    "                        comma = j; break\n",
    "                    j += 1\n",
    "                depth2, k = 0, comma + 1\n",
    "                while k < len(text):\n",
    "                    if text[k] == \"(\":\n",
    "                        depth2 += 1\n",
    "                    elif text[k] == \")\":\n",
    "                        if depth2 == 0: break\n",
    "                        depth2 -= 1\n",
    "                    k += 1\n",
    "                a = text[idx + len(token) + 1:comma]\n",
    "                b = text[comma + 1:k]\n",
    "                repl = \"(\" + a.strip() + f\" {op} \" + b.strip() + \")\"\n",
    "                text = text[:idx] + repl + text[k+1:]\n",
    "            return text\n",
    "        s = bin_to_infix(s, \"ADD\", \"+\")\n",
    "        s = bin_to_infix(s, \"SUB\", \"-\")\n",
    "        s = bin_to_infix(s, \"MUL\", \"*\")\n",
    "        return s\n",
    "\n",
    "    expr_str = str(gp._program)\n",
    "    eq_py = program_to_python(expr_str, FEATURES)\n",
    "\n",
    "    # Evaluate safely\n",
    "    def safe_eval_py(expr, Xmat, names):\n",
    "        env = {\n",
    "            \"np\": np,\n",
    "            \"pdiv\": lambda a,b: a/np.where(np.abs(b)<1e-9, np.sign(b)*1e-9 + (b==0)*1e-9, b),\n",
    "            \"psqrt\": lambda x: np.sqrt(np.abs(x)),\n",
    "            \"plog\":  lambda x: np.log1p(np.abs(x)),\n",
    "            \"pexp\":  lambda x: np.exp(np.clip(x, -20, 20)),\n",
    "        }\n",
    "        vals = {n: Xmat[:, i] for i, n in enumerate(names)}\n",
    "        return np.asarray(eval(expr, env, vals), float)\n",
    "\n",
    "    yhat_tr = safe_eval_py(eq_py, X_tr, FEATURES)\n",
    "    yhat_te = safe_eval_py(eq_py, X_te, FEATURES)\n",
    "\n",
    "    used_backend = \"gplearn\"\n",
    "    save_results(\"GP-Fallback\", eq_py, yhat_tr, yhat_te)\n",
    "\n",
    "print(f\"\\nArtifacts saved in: {os.path.abspath(OUTDIR)}\")\n",
    "print(\"Backend used:\", used_backend or \"unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36be204-72b7-426a-b328-1bdc3d25762b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
